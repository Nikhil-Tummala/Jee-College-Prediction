{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f8f38b2",
   "metadata": {},
   "source": [
    "# JEE College Prediction - Comprehensive Data Analysis\n",
    "\n",
    "This notebook provides a complete analysis of the JEE College Prediction dataset, including data loading, cleaning, exploratory data analysis, feature engineering, model training, and evaluation.\n",
    "\n",
    "## Table of Contents`\n",
    "1. [Import Required Libraries](#import-libraries)\n",
    "2. [Load the Dataset with Error Handling](#load-data)\n",
    "3. [Display Dataset Information](#dataset-info)\n",
    "4. [Data Cleaning: Handle Missing Values](#missing-values)\n",
    "5. [Data Cleaning: Clean Rank Columns](#clean-ranks)\n",
    "6. [Save Cleaned Data](#save-cleaned)\n",
    "7. [Exploratory Data Analysis: Basic Statistics](#basic-stats)\n",
    "8. [Exploratory Data Analysis: Visualizations](#visualizations)\n",
    "9. [Exploratory Data Analysis: Correlation Analysis](#correlation)\n",
    "10. [Feature Engineering: Define Features and Targets](#define-features)\n",
    "11. [Feature Engineering: Encode Target Variables](#encode-targets)\n",
    "12. [Feature Engineering: Define Feature Types](#feature-types)\n",
    "13. [Model Training: Create Preprocessing Pipeline](#preprocessing)\n",
    "14. [Model Training: Train/Test Split](#train-test-split)\n",
    "15. [Model Training: Train the Model](#train-model)\n",
    "16. [Model Evaluation: Accuracy Metrics](#accuracy)\n",
    "17. [Model Evaluation: Feature Importance](#feature-importance)\n",
    "18. [Save Trained Model](#save-model)\n",
    "19. [Conclusions and Recommendations](#conclusions)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b888d54b",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries {#import-libraries}\n",
    "\n",
    "Let's start by importing all the necessary libraries for data analysis, visualization, and machine learning. We'll also configure the visualization settings for better plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1208d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries for data analysis and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# Configure warnings and display settings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Set visualization style for better plots\n",
    "plt.style.use('default')  # Use default instead of seaborn-v0_8 for compatibility\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "print(f\"📊 Pandas version: {pd.__version__}\")\n",
    "print(f\"🔢 NumPy version: {np.__version__}\")\n",
    "print(f\"📈 Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(f\"🎨 Seaborn version: {sns.__version__}\")\n",
    "print(f\"🤖 Scikit-learn version: {sklearn.__version__}\")\n",
    "\n",
    "# Import scikit-learn for version check\n",
    "import sklearn\n",
    "print(f\"🗓️  Analysis started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b183709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries for data analysis and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Import machine learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Import serialization libraries\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "# Configure warnings and display settings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Set visualization style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "print(f\"📊 Pandas version: {pd.__version__}\")\n",
    "print(f\"🔢 NumPy version: {np.__version__}\")\n",
    "print(f\"📈 Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(f\"🎨 Seaborn version: {sns.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d58900",
   "metadata": {},
   "source": [
    "## 2. Load the Dataset with Error Handling {#load-data}\n",
    "\n",
    "Let's load the JEE admission dataset with proper error handling to ensure the notebook continues gracefully even if the data file is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c1b4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "data_dir = Path(\"../data/raw\")\n",
    "processed_dir = Path(\"../data/processed\")\n",
    "models_dir = Path(\"../models\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Initialize dataset variable\n",
    "final_df = None\n",
    "data_loaded = False\n",
    "\n",
    "# Try to load the dataset with comprehensive error handling\n",
    "print(\"🔍 Attempting to load dataset...\")\n",
    "\n",
    "try:\n",
    "    # Try different possible data file locations and names\n",
    "    possible_files = [\n",
    "        data_dir / \"data_v1.pkl\",\n",
    "        data_dir / \"jee_data.pkl\",\n",
    "        data_dir / \"dataset.pkl\",\n",
    "        processed_dir / \"data_v2.pkl\"\n",
    "    ]\n",
    "    \n",
    "    for file_path in possible_files:\n",
    "        if file_path.exists():\n",
    "            print(f\"📁 Found data file at: {file_path}\")\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                final_df = pickle.load(f)\n",
    "            data_loaded = True\n",
    "            break\n",
    "    \n",
    "    if data_loaded:\n",
    "        print(\"✅ Data loaded successfully!\")\n",
    "        print(f\"📊 Dataset shape: {final_df.shape}\")\n",
    "        print(f\"📋 Columns: {final_df.columns.tolist()}\")\n",
    "        print(f\"💾 Memory usage: {final_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    else:\n",
    "        print(\"❌ No data file found in expected locations:\")\n",
    "        for file_path in possible_files:\n",
    "            print(f\"   - {file_path}\")\n",
    "        print(\"\\n🔧 Please ensure the data file exists in one of these locations.\")\n",
    "        print(\"📝 You can also create sample data using the data generation script.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading data: {str(e)}\")\n",
    "    print(f\"🔧 Error type: {type(e).__name__}\")\n",
    "    data_loaded = False\n",
    "\n",
    "# Set a flag for subsequent cells\n",
    "DATA_AVAILABLE = data_loaded\n",
    "print(f\"\\n🎯 Data availability status: {'✅ Available' if DATA_AVAILABLE else '❌ Not Available'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67e9f0a",
   "metadata": {},
   "source": [
    "## 3. Display Dataset Information {#dataset-info}\n",
    "\n",
    "Let's examine the structure and basic information about our dataset to better understand what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275f7f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only proceed if data is available\n",
    "if DATA_AVAILABLE and final_df is not None:\n",
    "    print(\"📊 DATASET OVERVIEW\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Basic information\n",
    "    print(f\"📏 Dataset Shape: {final_df.shape}\")\n",
    "    print(f\"📋 Number of Columns: {len(final_df.columns)}\")\n",
    "    print(f\"📄 Number of Rows: {len(final_df)}\")\n",
    "    \n",
    "    print(\"\\n📋 COLUMN INFORMATION\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Column Names:\")\n",
    "    for i, col in enumerate(final_df.columns, 1):\n",
    "        print(f\"  {i:2d}. {col}\")\n",
    "    \n",
    "    print(\"\\n📈 DATA TYPES\")\n",
    "    print(\"=\" * 50)\n",
    "    print(final_df.dtypes)\n",
    "    \n",
    "    print(\"\\n📊 DATASET INFO\")\n",
    "    print(\"=\" * 50)\n",
    "    final_df.info()\n",
    "    \n",
    "    print(\"\\n👀 FIRST 5 ROWS\")\n",
    "    print(\"=\" * 50)\n",
    "    display(final_df.head())\n",
    "    \n",
    "    print(\"\\n🔍 LAST 5 ROWS\")\n",
    "    print(\"=\" * 50)\n",
    "    display(final_df.tail())\n",
    "    \n",
    "    print(\"\\n📊 BASIC STATISTICS\")\n",
    "    print(\"=\" * 50)\n",
    "    display(final_df.describe(include='all'))\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️  Skipping dataset information display - No data available\")\n",
    "    print(\"📝 Please ensure the dataset is loaded before proceeding with analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768fce05",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning: Handle Missing Values {#missing-values}\n",
    "\n",
    "Data cleaning is crucial for accurate analysis. Let's identify and handle missing values appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffea1b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only proceed if data is available\n",
    "if DATA_AVAILABLE and final_df is not None:\n",
    "    print(\"🔍 MISSING VALUES ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = final_df.isnull().sum()\n",
    "    missing_percentage = (missing_values / len(final_df)) * 100\n",
    "    \n",
    "    # Create a summary of missing values\n",
    "    missing_summary = pd.DataFrame({\n",
    "        'Column': missing_values.index,\n",
    "        'Missing_Count': missing_values.values,\n",
    "        'Missing_Percentage': missing_percentage.values\n",
    "    })\n",
    "    \n",
    "    # Filter only columns with missing values\n",
    "    missing_summary = missing_summary[missing_summary['Missing_Count'] > 0]\n",
    "    \n",
    "    if len(missing_summary) > 0:\n",
    "        print(\"📊 Missing Values Summary:\")\n",
    "        display(missing_summary.sort_values('Missing_Count', ascending=False))\n",
    "        \n",
    "        # Handle missing values based on column type\n",
    "        original_shape = final_df.shape\n",
    "        \n",
    "        # 1. Handle missing Institute values (critical for analysis)\n",
    "        if 'Institute' in final_df.columns:\n",
    "            institute_missing = final_df['Institute'].isnull().sum()\n",
    "            if institute_missing > 0:\n",
    "                print(f\"\\n🏫 Removing {institute_missing} rows with missing Institute information...\")\n",
    "                final_df = final_df.dropna(subset=['Institute'])\n",
    "                print(f\"   Dataset shape after removal: {final_df.shape}\")\n",
    "        \n",
    "        # 2. Handle missing Gender values\n",
    "        if 'Gender' in final_df.columns:\n",
    "            gender_missing = final_df['Gender'].isnull().sum()\n",
    "            if gender_missing > 0:\n",
    "                print(f\"\\n👥 Filling {gender_missing} missing Gender values with 'Neutral'...\")\n",
    "                final_df['Gender'] = final_df['Gender'].fillna('Neutral')\n",
    "                print(f\"   Gender distribution after filling:\")\n",
    "                print(final_df['Gender'].value_counts())\n",
    "        \n",
    "        # 3. Handle other missing values\n",
    "        for col in final_df.columns:\n",
    "            if col not in ['Institute', 'Gender']:\n",
    "                missing_count = final_df[col].isnull().sum()\n",
    "                if missing_count > 0:\n",
    "                    if final_df[col].dtype in ['object', 'category']:\n",
    "                        final_df[col] = final_df[col].fillna('Unknown')\n",
    "                        print(f\"\\n📝 Filled {missing_count} missing values in '{col}' with 'Unknown'\")\n",
    "                    else:\n",
    "                        median_val = final_df[col].median()\n",
    "                        final_df[col] = final_df[col].fillna(median_val)\n",
    "                        print(f\"\\n🔢 Filled {missing_count} missing values in '{col}' with median: {median_val}\")\n",
    "        \n",
    "        print(f\"\\n✅ Data cleaning completed!\")\n",
    "        print(f\"📏 Original shape: {original_shape}\")\n",
    "        print(f\"📏 Final shape: {final_df.shape}\")\n",
    "        \n",
    "        # Verify no missing values remain\n",
    "        remaining_missing = final_df.isnull().sum().sum()\n",
    "        print(f\"🎯 Remaining missing values: {remaining_missing}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"✅ No missing values found in the dataset!\")\n",
    "        \n",
    "else:\n",
    "    print(\"⚠️  Skipping missing values analysis - No data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182e199c",
   "metadata": {},
   "source": [
    "## 5. Data Cleaning: Clean Rank Columns {#clean-ranks}\n",
    "\n",
    "The rank columns might contain non-numeric values that need to be cleaned and converted to proper numeric format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b00b878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only proceed if data is available\n",
    "if DATA_AVAILABLE and final_df is not None:\n",
    "    print(\"🔧 CLEANING RANK COLUMNS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    def clean_rank(value):\n",
    "        \"\"\"\n",
    "        Clean rank data by converting various formats to integers.\n",
    "        \n",
    "        Args:\n",
    "            value: Raw rank value (could be string, float, or int)\n",
    "            \n",
    "        Returns:\n",
    "            int: Cleaned rank value or NaN if invalid\n",
    "        \"\"\"\n",
    "        if pd.isna(value):\n",
    "            return np.nan\n",
    "            \n",
    "        try:\n",
    "            # If it's already a number, try to convert directly\n",
    "            if isinstance(value, (int, float)):\n",
    "                return int(value) if not np.isnan(value) else np.nan\n",
    "            \n",
    "            # If it's a string, clean it\n",
    "            if isinstance(value, str):\n",
    "                # Remove whitespace and convert to lowercase\n",
    "                value = value.strip().lower()\n",
    "                \n",
    "                # Handle common string representations\n",
    "                if value in ['', 'nan', 'none', 'null', 'na']:\n",
    "                    return np.nan\n",
    "                \n",
    "                # Remove non-numeric characters except digits and decimal point\n",
    "                import re\n",
    "                cleaned = re.sub(r'[^\\d.]', '', value)\n",
    "                \n",
    "                if cleaned == '':\n",
    "                    return np.nan\n",
    "                \n",
    "                # Convert to float first, then to int\n",
    "                return int(float(cleaned))\n",
    "                \n",
    "        except (ValueError, TypeError, AttributeError):\n",
    "            return np.nan\n",
    "        \n",
    "        return np.nan\n",
    "    \n",
    "    # Define rank columns to clean\n",
    "    rank_columns = []\n",
    "    possible_rank_cols = ['Opening Rank', 'Closing Rank', 'opening_rank', 'closing_rank', \n",
    "                         'OpeningRank', 'ClosingRank', 'rank_opening', 'rank_closing']\n",
    "    \n",
    "    for col in possible_rank_cols:\n",
    "        if col in final_df.columns:\n",
    "            rank_columns.append(col)\n",
    "    \n",
    "    if rank_columns:\n",
    "        print(f\"🎯 Found rank columns: {rank_columns}\")\n",
    "        \n",
    "        for col in rank_columns:\n",
    "            print(f\"\\n🔧 Cleaning column: {col}\")\n",
    "            \n",
    "            # Show original data type and sample values\n",
    "            print(f\"   Original dtype: {final_df[col].dtype}\")\n",
    "            print(f\"   Original sample values: {final_df[col].head().tolist()}\")\n",
    "            \n",
    "            # Count invalid values before cleaning\n",
    "            if final_df[col].dtype == 'object':\n",
    "                non_numeric = final_df[col].apply(lambda x: not str(x).replace('.', '').replace('-', '').isdigit() if pd.notna(x) else False).sum()\n",
    "                print(f\"   Non-numeric values before cleaning: {non_numeric}\")\n",
    "            \n",
    "            # Apply cleaning function\n",
    "            original_non_null = final_df[col].notna().sum()\n",
    "            final_df[col] = final_df[col].apply(clean_rank)\n",
    "            cleaned_non_null = final_df[col].notna().sum()\n",
    "            \n",
    "            # Show results\n",
    "            print(f\"   ✅ Cleaned successfully!\")\n",
    "            print(f\"   📊 Valid values: {original_non_null} → {cleaned_non_null}\")\n",
    "            print(f\"   📈 Data type: {final_df[col].dtype}\")\n",
    "            \n",
    "            if cleaned_non_null > 0:\n",
    "                print(f\"   📊 Min: {final_df[col].min()}\")\n",
    "                print(f\"   📊 Max: {final_df[col].max()}\")\n",
    "                print(f\"   📊 Mean: {final_df[col].mean():.2f}\")\n",
    "                print(f\"   📊 Median: {final_df[col].median():.2f}\")\n",
    "            \n",
    "            # Show cleaned sample values\n",
    "            print(f\"   📋 Cleaned sample values: {final_df[col].dropna().head().tolist()}\")\n",
    "        \n",
    "        print(f\"\\n✅ All rank columns cleaned successfully!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"⚠️  No rank columns found in the dataset\")\n",
    "        print(\"📋 Available columns:\")\n",
    "        for col in final_df.columns:\n",
    "            print(f\"   - {col}\")\n",
    "            \n",
    "else:\n",
    "    print(\"⚠️  Skipping rank column cleaning - No data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9ccf2c",
   "metadata": {},
   "source": [
    "## 6. Save Cleaned Data {#save-cleaned}\n",
    "\n",
    "Let's save the cleaned dataset for future use and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e2c44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only proceed if data is available\n",
    "if DATA_AVAILABLE and final_df is not None:\n",
    "    print(\"💾 SAVING CLEANED DATA\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Define output path\n",
    "        output_path = processed_dir / \"cleaned_data.pkl\"\n",
    "        \n",
    "        # Save the cleaned dataset\n",
    "        with open(output_path, \"wb\") as f:\n",
    "            pickle.dump(final_df, f)\n",
    "        \n",
    "        print(f\"✅ Cleaned data saved successfully!\")\n",
    "        print(f\"📁 Location: {output_path}\")\n",
    "        print(f\"📊 Shape: {final_df.shape}\")\n",
    "        print(f\"💾 File size: {output_path.stat().st_size / 1024**2:.2f} MB\")\n",
    "        \n",
    "        # Also save as CSV for easy inspection\n",
    "        csv_path = processed_dir / \"cleaned_data.csv\"\n",
    "        final_df.to_csv(csv_path, index=False)\n",
    "        print(f\"📄 CSV version saved: {csv_path}\")\n",
    "        \n",
    "        # Create a data summary\n",
    "        summary = {\n",
    "            'timestamp': pd.Timestamp.now().isoformat(),\n",
    "            'shape': final_df.shape,\n",
    "            'columns': final_df.columns.tolist(),\n",
    "            'dtypes': final_df.dtypes.to_dict(),\n",
    "            'missing_values': final_df.isnull().sum().to_dict(),\n",
    "            'memory_usage_mb': final_df.memory_usage(deep=True).sum() / 1024**2\n",
    "        }\n",
    "        \n",
    "        summary_path = processed_dir / \"data_summary.json\"\n",
    "        import json\n",
    "        with open(summary_path, 'w') as f:\n",
    "            json.dump(summary, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"📋 Data summary saved: {summary_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving cleaned data: {str(e)}\")\n",
    "        print(f\"🔧 Error type: {type(e).__name__}\")\n",
    "        \n",
    "else:\n",
    "    print(\"⚠️  Skipping data save - No data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13004222",
   "metadata": {},
   "source": [
    "## 7. Exploratory Data Analysis: Basic Statistics {#basic-stats}\n",
    "\n",
    "Let's examine the statistical properties of our cleaned dataset to understand the data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4521b901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only proceed if data is available\n",
    "if DATA_AVAILABLE and final_df is not None:\n",
    "    print(\"📊 BASIC STATISTICS ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Separate numeric and categorical columns\n",
    "    numeric_cols = final_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = final_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    print(f\"🔢 Numeric columns ({len(numeric_cols)}): {numeric_cols}\")\n",
    "    print(f\"📝 Categorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
    "    \n",
    "    # Numeric statistics\n",
    "    if numeric_cols:\n",
    "        print(f\"\\n📊 NUMERIC COLUMNS STATISTICS\")\n",
    "        print(\"-\" * 40)\n",
    "        display(final_df[numeric_cols].describe())\n",
    "        \n",
    "        # Additional statistics for numeric columns\n",
    "        print(f\"\\n📈 ADDITIONAL NUMERIC STATISTICS\")\n",
    "        print(\"-\" * 40)\n",
    "        for col in numeric_cols:\n",
    "            print(f\"\\n📊 {col}:\")\n",
    "            print(f\"   Count: {final_df[col].count()}\")\n",
    "            print(f\"   Mean: {final_df[col].mean():.2f}\")\n",
    "            print(f\"   Median: {final_df[col].median():.2f}\")\n",
    "            print(f\"   Mode: {final_df[col].mode().iloc[0] if not final_df[col].mode().empty else 'N/A'}\")\n",
    "            print(f\"   Std Dev: {final_df[col].std():.2f}\")\n",
    "            print(f\"   Variance: {final_df[col].var():.2f}\")\n",
    "            print(f\"   Min: {final_df[col].min()}\")\n",
    "            print(f\"   Max: {final_df[col].max()}\")\n",
    "            print(f\"   Range: {final_df[col].max() - final_df[col].min()}\")\n",
    "            print(f\"   Skewness: {final_df[col].skew():.2f}\")\n",
    "            print(f\"   Kurtosis: {final_df[col].kurtosis():.2f}\")\n",
    "    \n",
    "    # Categorical statistics\n",
    "    if categorical_cols:\n",
    "        print(f\"\\n📝 CATEGORICAL COLUMNS STATISTICS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            print(f\"\\n📊 {col}:\")\n",
    "            print(f\"   Unique values: {final_df[col].nunique()}\")\n",
    "            print(f\"   Most frequent: {final_df[col].mode().iloc[0] if not final_df[col].mode().empty else 'N/A'}\")\n",
    "            print(f\"   Top 5 values:\")\n",
    "            \n",
    "            value_counts = final_df[col].value_counts().head()\n",
    "            for idx, (value, count) in enumerate(value_counts.items(), 1):\n",
    "                percentage = (count / len(final_df)) * 100\n",
    "                print(f\"     {idx}. {value}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Data quality check\n",
    "    print(f\"\\n🔍 DATA QUALITY CHECK\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"📊 Total records: {len(final_df)}\")\n",
    "    print(f\"📊 Complete records: {len(final_df.dropna())}\")\n",
    "    print(f\"📊 Records with missing values: {len(final_df) - len(final_df.dropna())}\")\n",
    "    print(f\"📊 Duplicate records: {final_df.duplicated().sum()}\")\n",
    "    \n",
    "    # Memory usage\n",
    "    print(f\"\\n💾 MEMORY USAGE\")\n",
    "    print(\"-\" * 40)\n",
    "    memory_usage = final_df.memory_usage(deep=True)\n",
    "    print(f\"📊 Total memory usage: {memory_usage.sum() / 1024**2:.2f} MB\")\n",
    "    print(f\"📊 Average per column: {memory_usage.mean() / 1024**2:.2f} MB\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️  Skipping basic statistics analysis - No data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf09b57",
   "metadata": {},
   "source": [
    "## 8. Exploratory Data Analysis: Visualizations {#visualizations}\n",
    "\n",
    "Visual exploration helps us understand patterns, distributions, and relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ba79f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only proceed if data is available\n",
    "if DATA_AVAILABLE and final_df is not None:\n",
    "    print(\"📊 DATA VISUALIZATIONS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get column information\n",
    "    numeric_cols = final_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = final_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    # 1. Categorical Variables Distribution\n",
    "    if categorical_cols:\n",
    "        print(\"📊 Categorical Variables Distribution\")\n",
    "        \n",
    "        # Determine subplot layout\n",
    "        n_cats = len(categorical_cols)\n",
    "        if n_cats <= 4:\n",
    "            cols = 2\n",
    "            rows = (n_cats + 1) // 2\n",
    "        else:\n",
    "            cols = 3\n",
    "            rows = (n_cats + 2) // 3\n",
    "        \n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))\n",
    "        \n",
    "        # Handle single subplot case\n",
    "        if rows == 1 and cols == 1:\n",
    "            axes = [axes]\n",
    "        elif rows == 1 or cols == 1:\n",
    "            axes = axes.flatten()\n",
    "        else:\n",
    "            axes = axes.flatten()\n",
    "        \n",
    "        for i, col in enumerate(categorical_cols):\n",
    "            if i < len(axes):\n",
    "                try:\n",
    "                    # Get top 10 values to avoid cluttered plots\n",
    "                    top_values = final_df[col].value_counts().head(10)\n",
    "                    \n",
    "                    if len(top_values) > 0:\n",
    "                        top_values.plot(kind='bar', ax=axes[i], color='skyblue')\n",
    "                        axes[i].set_title(f'{col} Distribution (Top 10)')\n",
    "                        axes[i].set_xlabel(col)\n",
    "                        axes[i].set_ylabel('Count')\n",
    "                        axes[i].tick_params(axis='x', rotation=45)\n",
    "                        \n",
    "                        # Add value labels on bars\n",
    "                        for j, (val, count) in enumerate(top_values.items()):\n",
    "                            axes[i].text(j, count + max(top_values) * 0.01, str(count), \n",
    "                                       ha='center', va='bottom', fontsize=10)\n",
    "                    else:\n",
    "                        axes[i].text(0.5, 0.5, f'No data for {col}', \n",
    "                                   ha='center', va='center', transform=axes[i].transAxes)\n",
    "                        axes[i].set_title(f'{col} - No Data')\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    axes[i].text(0.5, 0.5, f'Error plotting {col}', \n",
    "                               ha='center', va='center', transform=axes[i].transAxes)\n",
    "                    print(f\"Warning: Error plotting {col}: {e}\")\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for i in range(len(categorical_cols), len(axes)):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # 2. Numeric Variables Distribution\n",
    "    if numeric_cols:\n",
    "        print(\"📊 Numeric Variables Distribution\")\n",
    "        \n",
    "        n_nums = len(numeric_cols)\n",
    "        if n_nums <= 4:\n",
    "            cols = 2\n",
    "            rows = (n_nums + 1) // 2\n",
    "        else:\n",
    "            cols = 3\n",
    "            rows = (n_nums + 2) // 3\n",
    "        \n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))\n",
    "        \n",
    "        # Handle single subplot case\n",
    "        if rows == 1 and cols == 1:\n",
    "            axes = [axes]\n",
    "        elif rows == 1 or cols == 1:\n",
    "            axes = axes.flatten()\n",
    "        else:\n",
    "            axes = axes.flatten()\n",
    "        \n",
    "        for i, col in enumerate(numeric_cols):\n",
    "            if i < len(axes):\n",
    "                try:\n",
    "                    # Remove NaN values for plotting\n",
    "                    data = final_df[col].dropna()\n",
    "                    \n",
    "                    if len(data) > 0:\n",
    "                        # Create histogram\n",
    "                        axes[i].hist(data, bins=30, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "                        axes[i].set_title(f'{col} Distribution')\n",
    "                        axes[i].set_xlabel(col)\n",
    "                        axes[i].set_ylabel('Frequency')\n",
    "                        \n",
    "                        # Add statistics text\n",
    "                        mean_val = data.mean()\n",
    "                        median_val = data.median()\n",
    "                        std_val = data.std()\n",
    "                        \n",
    "                        stats_text = f'Mean: {mean_val:.2f}\\\\nMedian: {median_val:.2f}\\\\nStd: {std_val:.2f}'\n",
    "                        axes[i].text(0.02, 0.98, stats_text, transform=axes[i].transAxes, \n",
    "                                   verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "                    else:\n",
    "                        axes[i].text(0.5, 0.5, f'No data for {col}', \n",
    "                                   ha='center', va='center', transform=axes[i].transAxes)\n",
    "                        axes[i].set_title(f'{col} - No Data')\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    axes[i].text(0.5, 0.5, f'Error plotting {col}', \n",
    "                               ha='center', va='center', transform=axes[i].transAxes)\n",
    "                    print(f\"Warning: Error plotting {col}: {e}\")\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for i in range(len(numeric_cols), len(axes)):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # 3. Box plots for numeric variables\n",
    "    if numeric_cols and len(numeric_cols) > 1:\n",
    "        print(\"📊 Box Plots for Numeric Variables\")\n",
    "        \n",
    "        fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "        \n",
    "        try:\n",
    "            # Create box plot\n",
    "            final_df[numeric_cols].plot(kind='box', ax=ax)\n",
    "            ax.set_title('Box Plot of Numeric Variables')\n",
    "            ax.set_ylabel('Values')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error creating box plot: {e}\")\n",
    "    \n",
    "    print(\"✅ Visualizations completed successfully!\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️  Skipping data visualizations - No data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f76600",
   "metadata": {},
   "source": [
    "## 9. Exploratory Data Analysis: Correlation Analysis {#correlation}\n",
    "\n",
    "Understanding correlations between variables helps identify relationships and potential predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45517cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only proceed if data is available\n",
    "if DATA_AVAILABLE and final_df is not None:\n",
    "    print(\"📊 CORRELATION ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get numeric columns for correlation analysis\n",
    "    numeric_cols = final_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    if len(numeric_cols) > 1:\n",
    "        print(f\"🔢 Analyzing correlations for {len(numeric_cols)} numeric columns\")\n",
    "        \n",
    "        # Calculate correlation matrix\n",
    "        correlation_matrix = final_df[numeric_cols].corr()\n",
    "        \n",
    "        print(\"\\n📊 CORRELATION MATRIX\")\n",
    "        print(\"-\" * 40)\n",
    "        display(correlation_matrix)\n",
    "        \n",
    "        # Create correlation heatmap\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # Create a mask for the upper triangle\n",
    "        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "        \n",
    "        # Create heatmap\n",
    "        sns.heatmap(correlation_matrix, \n",
    "                   annot=True, \n",
    "                   cmap='coolwarm', \n",
    "                   center=0,\n",
    "                   square=True,\n",
    "                   mask=mask,\n",
    "                   fmt='.2f',\n",
    "                   cbar_kws={'label': 'Correlation Coefficient'})\n",
    "        \n",
    "        plt.title('Correlation Matrix Heatmap', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Find strong correlations (>0.7 or <-0.7)\n",
    "        print(\"\\n🔍 STRONG CORRELATIONS (|r| > 0.7)\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        strong_correlations = []\n",
    "        for i in range(len(correlation_matrix.columns)):\n",
    "            for j in range(i+1, len(correlation_matrix.columns)):\n",
    "                corr_val = correlation_matrix.iloc[i, j]\n",
    "                if abs(corr_val) > 0.7:\n",
    "                    strong_correlations.append({\n",
    "                        'Variable 1': correlation_matrix.columns[i],\n",
    "                        'Variable 2': correlation_matrix.columns[j],\n",
    "                        'Correlation': corr_val\n",
    "                    })\n",
    "        \n",
    "        if strong_correlations:\n",
    "            strong_corr_df = pd.DataFrame(strong_correlations)\n",
    "            strong_corr_df = strong_corr_df.sort_values('Correlation', key=abs, ascending=False)\n",
    "            display(strong_corr_df)\n",
    "        else:\n",
    "            print(\"No strong correlations found (|r| > 0.7)\")\n",
    "        \n",
    "        # Find moderate correlations (0.3 < |r| < 0.7)\n",
    "        print(\"\\n🔍 MODERATE CORRELATIONS (0.3 < |r| < 0.7)\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        moderate_correlations = []\n",
    "        for i in range(len(correlation_matrix.columns)):\n",
    "            for j in range(i+1, len(correlation_matrix.columns)):\n",
    "                corr_val = correlation_matrix.iloc[i, j]\n",
    "                if 0.3 < abs(corr_val) < 0.7:\n",
    "                    moderate_correlations.append({\n",
    "                        'Variable 1': correlation_matrix.columns[i],\n",
    "                        'Variable 2': correlation_matrix.columns[j],\n",
    "                        'Correlation': corr_val\n",
    "                    })\n",
    "        \n",
    "        if moderate_correlations:\n",
    "            moderate_corr_df = pd.DataFrame(moderate_correlations)\n",
    "            moderate_corr_df = moderate_corr_df.sort_values('Correlation', key=abs, ascending=False)\n",
    "            display(moderate_corr_df.head(10))  # Show top 10\n",
    "        else:\n",
    "            print(\"No moderate correlations found (0.3 < |r| < 0.7)\")\n",
    "        \n",
    "        # Correlation insights\n",
    "        print(\"\\n💡 CORRELATION INSIGHTS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Check for multicollinearity\n",
    "        high_corr_pairs = len([c for c in strong_correlations if abs(c['Correlation']) > 0.9])\n",
    "        if high_corr_pairs > 0:\n",
    "            print(f\"⚠️  {high_corr_pairs} pairs with very high correlation (>0.9) - potential multicollinearity\")\n",
    "        \n",
    "        # Summary statistics\n",
    "        corr_values = correlation_matrix.values\n",
    "        upper_triangle = corr_values[np.triu_indices_from(corr_values, k=1)]\n",
    "        \n",
    "        print(f\"📊 Average correlation: {np.mean(np.abs(upper_triangle)):.3f}\")\n",
    "        print(f\"📊 Maximum correlation: {np.max(np.abs(upper_triangle)):.3f}\")\n",
    "        print(f\"📊 Minimum correlation: {np.min(np.abs(upper_triangle)):.3f}\")\n",
    "        print(f\"📊 Standard deviation: {np.std(upper_triangle):.3f}\")\n",
    "        \n",
    "    elif len(numeric_cols) == 1:\n",
    "        print(f\"⚠️  Only one numeric column found: {numeric_cols[0]}\")\n",
    "        print(\"Cannot perform correlation analysis with single variable\")\n",
    "    else:\n",
    "        print(\"⚠️  No numeric columns found for correlation analysis\")\n",
    "        print(\"Available columns:\")\n",
    "        for col in final_df.columns:\n",
    "            print(f\"   - {col}: {final_df[col].dtype}\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️  Skipping correlation analysis - No data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32af09d5",
   "metadata": {},
   "source": [
    "## 10. Feature Engineering: Define Features and Targets {#define-features}\n",
    "\n",
    "Now let's prepare our data for machine learning by defining features (input variables) and targets (output variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7a7ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only proceed if data is available\n",
    "if DATA_AVAILABLE and final_df is not None:\n",
    "    print(\"🎯 FEATURE ENGINEERING: DEFINE FEATURES AND TARGETS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Define potential feature columns (input variables)\n",
    "    potential_features = [\n",
    "        'Opening Rank', 'opening_rank', 'OpeningRank',\n",
    "        'Closing Rank', 'closing_rank', 'ClosingRank',\n",
    "        'Gender', 'gender',\n",
    "        'Seat Type', 'seat_type', 'SeatType',\n",
    "        'Category', 'category',\n",
    "        'State', 'state',\n",
    "        'Quota', 'quota',\n",
    "        'Branch', 'branch',\n",
    "        'year'\n",
    "    ]\n",
    "    \n",
    "    # Find available feature columns\n",
    "    available_features = []\n",
    "    for col in potential_features:\n",
    "        if col in final_df.columns:\n",
    "            available_features.append(col)\n",
    "    \n",
    "    print(f\"📋 Available columns in dataset: {final_df.columns.tolist()}\")\n",
    "    print(f\"🎯 Potential feature columns found: {available_features}\")\n",
    "    \n",
    "    # Define potential target columns (output variables)\n",
    "    potential_targets = [\n",
    "        'Institute', 'institute',\n",
    "        'round', 'Round',\n",
    "        'Branch', 'branch',\n",
    "        'admission_status', 'status'\n",
    "    ]\n",
    "    \n",
    "    # Find available target columns\n",
    "    available_targets = []\n",
    "    for col in potential_targets:\n",
    "        if col in final_df.columns:\n",
    "            available_targets.append(col)\n",
    "    \n",
    "    print(f\"🎯 Potential target columns found: {available_targets}\")\n",
    "    \n",
    "    # Select features and targets based on availability\n",
    "    if available_features and available_targets:\n",
    "        # Prioritize key features\n",
    "        selected_features = []\\n        if 'Opening Rank' in available_features:\\n            selected_features.append('Opening Rank')\\n        elif 'opening_rank' in available_features:\\n            selected_features.append('opening_rank')\\n        \\n        if 'Gender' in available_features:\\n            selected_features.append('Gender')\\n        elif 'gender' in available_features:\\n            selected_features.append('gender')\\n        \\n        if 'Seat Type' in available_features:\\n            selected_features.append('Seat Type')\\n        elif 'seat_type' in available_features:\\n            selected_features.append('seat_type')\\n        \\n        # Add other available features\\n        for feature in available_features:\\n            if feature not in selected_features:\\n                selected_features.append(feature)\\n        \\n        # Select targets\\n        selected_targets = []\\n        if 'Institute' in available_targets:\\n            selected_targets.append('Institute')\\n        elif 'institute' in available_targets:\\n            selected_targets.append('institute')\\n        \\n        if 'round' in available_targets:\\n            selected_targets.append('round')\\n        elif 'Round' in available_targets:\\n            selected_targets.append('Round')\\n        \\n        # Add other available targets\\n        for target in available_targets:\\n            if target not in selected_targets:\\n                selected_targets.append(target)\\n        \\n        # Remove any targets that are also in features\\n        selected_features = [f for f in selected_features if f not in selected_targets]\\n        \\n        # Create feature and target datasets\\n        try:\\n            # Filter out rows with missing values in key columns\\n            key_columns = selected_features + selected_targets\\n            clean_data = final_df[key_columns].dropna()\\n            \\n            X = clean_data[selected_features]\\n            y = clean_data[selected_targets]\\n            \\n            print(f\\\"\\\\n✅ FEATURE AND TARGET SELECTION SUCCESSFUL\\\")\\n            print(f\\\"📊 Features selected: {selected_features}\\\")\\n            print(f\\\"📊 Targets selected: {selected_targets}\\\")\\n            print(f\\\"📊 Features shape: {X.shape}\\\")\\n            print(f\\\"📊 Targets shape: {y.shape}\\\")\\n            print(f\\\"📊 Clean data shape: {clean_data.shape}\\\")\\n            \\n            # Show feature information\\n            print(f\\\"\\\\n📋 FEATURE INFORMATION\\\")\\n            print(\\\"-\\\" * 30)\\n            for i, feature in enumerate(selected_features, 1):\\n                dtype = X[feature].dtype\\n                unique_vals = X[feature].nunique()\\n                print(f\\\"{i:2d}. {feature}: {dtype} ({unique_vals} unique values)\\\")\\n            \\n            # Show target information\\n            print(f\\\"\\\\n🎯 TARGET INFORMATION\\\")\\n            print(\\\"-\\\" * 30)\\n            for i, target in enumerate(selected_targets, 1):\\n                dtype = y[target].dtype\\n                unique_vals = y[target].nunique()\\n                print(f\\\"{i:2d}. {target}: {dtype} ({unique_vals} unique values)\\\")\\n            \\n            # Sample data preview\\n            print(f\\\"\\\\n👀 SAMPLE DATA PREVIEW\\\")\\n            print(\\\"-\\\" * 30)\\n            print(\\\"Features (X):\\\")\\n            display(X.head())\\n            print(\\\"\\\\nTargets (y):\\\")\\n            display(y.head())\\n            \\n            # Set flags for next steps\\n            FEATURES_DEFINED = True\\n            print(f\\\"\\\\n✅ Features and targets defined successfully!\\\")\\n            \\n        except Exception as e:\\n            print(f\\\"❌ Error creating feature/target datasets: {str(e)}\\\")\\n            FEATURES_DEFINED = False\\n            X, y = None, None\\n    \\n    else:\\n        print(\\\"❌ Could not identify suitable features and targets\\\")\\n        print(\\\"📝 Please check your data structure and column names\\\")\\n        FEATURES_DEFINED = False\\n        X, y = None, None\\n        \\nelse:\\n    print(\\\"⚠️  Skipping feature definition - No data available\\\")\\n    FEATURES_DEFINED = False\\n    X, y = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341d8fea",
   "metadata": {},
   "source": [
    "## 11. Feature Engineering: Encode Target Variables {#encode-targets}\n",
    "\n",
    "Machine learning algorithms work with numeric data, so we need to encode categorical target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7555723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only proceed if features are defined\n",
    "if FEATURES_DEFINED and X is not None and y is not None:\n",
    "    print(\"🔢 ENCODING TARGET VARIABLES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Store original target data for reference\n",
    "    y_original = y.copy()\n",
    "    \n",
    "    # Dictionary to store encoders for each target\n",
    "    target_encoders = {}\\n    \\n    # Process each target column\\n    for target_col in y.columns:\\n        print(f\\\"\\\\n📊 Processing target: {target_col}\\\")\\n        \\n        # Check if target is already numeric\\n        if pd.api.types.is_numeric_dtype(y[target_col]):\\n            print(f\\\"   ✅ Already numeric: {y[target_col].dtype}\\\")\\n            print(f\\\"   📊 Unique values: {y[target_col].nunique()}\\\")\\n            print(f\\\"   📊 Range: {y[target_col].min()} to {y[target_col].max()}\\\")\\n            target_encoders[target_col] = None  # No encoding needed\\n        else:\\n            print(f\\\"   🔧 Encoding categorical target: {y[target_col].dtype}\\\")\\n            print(f\\\"   📊 Unique values before encoding: {y[target_col].nunique()}\\\")\\n            \\n            # Show top categories\\n            top_categories = y[target_col].value_counts().head(10)\\n            print(f\\\"   📋 Top categories:\\\")\\n            for cat, count in top_categories.items():\\n                print(f\\\"      - {cat}: {count}\\\")\\n            \\n            # Create and fit label encoder\\n            le = LabelEncoder()\\n            y[target_col] = le.fit_transform(y[target_col])\\n            target_encoders[target_col] = le\\n            \\n            print(f\\\"   ✅ Encoded successfully!\\\")\\n            print(f\\\"   📊 Unique values after encoding: {y[target_col].nunique()}\\\")\\n            print(f\\\"   📊 Encoded range: {y[target_col].min()} to {y[target_col].max()}\\\")\\n            \\n            # Show mapping for first few categories\\n            print(f\\\"   📋 Encoding mapping (first 10):\\\")\\n            for i, class_name in enumerate(le.classes_[:10]):\\n                print(f\\\"      {class_name} → {i}\\\")\\n            \\n            if len(le.classes_) > 10:\\n                print(f\\\"      ... and {len(le.classes_) - 10} more classes\\\")\\n    \\n    print(f\\\"\\\\n✅ TARGET ENCODING COMPLETED\\\")\\n    print(f\\\"📊 Encoded targets shape: {y.shape}\\\")\\n    print(f\\\"📊 All target columns are now numeric\\\")\\n    \\n    # Display encoded target information\\n    print(f\\\"\\\\n📋 ENCODED TARGET SUMMARY\\\")\\n    print(\\\"-\\\" * 40)\\n    for target_col in y.columns:\\n        print(f\\\"📊 {target_col}:\\\")\\n        print(f\\\"   Data type: {y[target_col].dtype}\\\")\\n        print(f\\\"   Unique values: {y[target_col].nunique()}\\\")\\n        print(f\\\"   Min value: {y[target_col].min()}\\\")\\n        print(f\\\"   Max value: {y[target_col].max()}\\\")\\n        print(f\\\"   Mean: {y[target_col].mean():.2f}\\\")\\n        print(f\\\"   Std: {y[target_col].std():.2f}\\\")\\n    \\n    # Show sample of encoded data\\n    print(f\\\"\\\\n👀 ENCODED TARGET SAMPLE\\\")\\n    print(\\\"-\\\" * 40)\\n    display(y.head(10))\\n    \\n    # Compare with original\\n    print(f\\\"\\\\n🔄 COMPARISON WITH ORIGINAL\\\")\\n    print(\\\"-\\\" * 40)\\n    comparison_df = pd.DataFrame()\\n    for target_col in y.columns:\\n        comparison_df[f'{target_col}_original'] = y_original[target_col].head(10)\\n        comparison_df[f'{target_col}_encoded'] = y[target_col].head(10)\\n    \\n    display(comparison_df)\\n    \\n    # Set flag for next steps\\n    TARGETS_ENCODED = True\\n    print(f\\\"\\\\n🎯 Target encoding completed successfully!\\\")\\n    \\nelse:\\n    print(\\\"⚠️  Skipping target encoding - Features not defined\\\")\\n    TARGETS_ENCODED = False\\n    target_encoders = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73fd683",
   "metadata": {},
   "source": [
    "## 12. Feature Engineering: Define Feature Types {#feature-types}\n",
    "\n",
    "We need to categorize our features as numerical or categorical for appropriate preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c75d6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only proceed if targets are encoded\n",
    "if TARGETS_ENCODED and X is not None:\n",
    "    print(\"🏷️ DEFINING FEATURE TYPES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Identify categorical and numerical features\n",
    "    categorical_features = []\\n    numerical_features = []\\n    \\n    for col in X.columns:\\n        if pd.api.types.is_numeric_dtype(X[col]):\\n            numerical_features.append(col)\\n        else:\\n            categorical_features.append(col)\\n    \\n    print(f\\\"📊 FEATURE TYPE CLASSIFICATION\\\")\\n    print(f\\\"🔢 Numerical features ({len(numerical_features)}): {numerical_features}\\\")\\n    print(f\\\"📝 Categorical features ({len(categorical_features)}): {categorical_features}\\\")\\n    \\n    # Show detailed information for each feature type\\n    if numerical_features:\\n        print(f\\\"\\\\n📊 NUMERICAL FEATURES DETAILS\\\")\\n        print(\\\"-\\\" * 40)\\n        for feature in numerical_features:\\n            print(f\\\"🔢 {feature}:\\\")\\n            print(f\\\"   Data type: {X[feature].dtype}\\\")\\n            print(f\\\"   Unique values: {X[feature].nunique()}\\\")\\n            print(f\\\"   Range: {X[feature].min()} to {X[feature].max()}\\\")\\n            print(f\\\"   Mean: {X[feature].mean():.2f}\\\")\\n            print(f\\\"   Std: {X[feature].std():.2f}\\\")\\n            print(f\\\"   Missing values: {X[feature].isnull().sum()}\\\")\\n            print()\\n    \\n    if categorical_features:\\n        print(f\\\"\\\\n📝 CATEGORICAL FEATURES DETAILS\\\")\\n        print(\\\"-\\\" * 40)\\n        for feature in categorical_features:\\n            print(f\\\"📝 {feature}:\\\")\\n            print(f\\\"   Data type: {X[feature].dtype}\\\")\\n            print(f\\\"   Unique values: {X[feature].nunique()}\\\")\\n            print(f\\\"   Missing values: {X[feature].isnull().sum()}\\\")\\n            print(f\\\"   Top 5 categories:\\\")\\n            \\n            top_categories = X[feature].value_counts().head(5)\\n            for cat, count in top_categories.items():\\n                percentage = (count / len(X)) * 100\\n                print(f\\\"      {cat}: {count} ({percentage:.1f}%)\\\")\\n            print()\\n    \\n    # Check for potential issues\\n    print(f\\\"📋 FEATURE QUALITY CHECK\\\")\\n    print(\\\"-\\\" * 40)\\n    \\n    # Check for high cardinality categorical features\\n    high_cardinality = []\\n    for feature in categorical_features:\\n        cardinality = X[feature].nunique()\\n        if cardinality > 50:  # Threshold for high cardinality\\n            high_cardinality.append((feature, cardinality))\\n    \\n    if high_cardinality:\\n        print(\\\"⚠️  High cardinality categorical features detected:\\\")\\n        for feature, cardinality in high_cardinality:\\n            print(f\\\"   - {feature}: {cardinality} unique values\\\")\\n        print(\\\"   Consider grouping rare categories or using target encoding\\\")\\n    else:\\n        print(\\\"✅ No high cardinality issues detected\\\")\\n    \\n    # Check for low variance numerical features\\n    low_variance = []\\n    for feature in numerical_features:\\n        if X[feature].std() < 0.1:  # Very low standard deviation\\n            low_variance.append((feature, X[feature].std()))\\n    \\n    if low_variance:\\n        print(\\\"⚠️  Low variance numerical features detected:\\\")\\n        for feature, std in low_variance:\\n            print(f\\\"   - {feature}: std = {std:.4f}\\\")\\n        print(\\\"   Consider removing or transforming these features\\\")\\n    else:\\n        print(\\\"✅ No low variance issues detected\\\")\\n    \\n    # Set flag for next steps\\n    FEATURE_TYPES_DEFINED = True\\n    print(f\\\"\\\\n🎯 Feature types defined successfully!\\\")\\n    print(f\\\"✅ Ready for preprocessing pipeline creation\\\")\\n    \\nelse:\\n    print(\\\"⚠️  Skipping feature type definition - Targets not encoded\\\")\\n    FEATURE_TYPES_DEFINED = False\\n    categorical_features = []\\n    numerical_features = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b94a846",
   "metadata": {},
   "source": [
    "## 13. Model Training: Create Preprocessing Pipeline {#preprocessing}\n",
    "\n",
    "Let's create a preprocessing pipeline that handles both numerical and categorical features appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859fae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only proceed if feature types are defined\n",
    "if FEATURE_TYPES_DEFINED and X is not None and y is not None:\n",
    "    print(\"🔧 CREATING PREPROCESSING PIPELINE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create preprocessing steps for numerical features\\n    numerical_transformer = StandardScaler()\\n    \\n    # Create preprocessing steps for categorical features\\n    categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\\n    \\n    # Create the column transformer\\n    preprocessor = ColumnTransformer(\\n        transformers=[\\n            ('num', numerical_transformer, numerical_features),\\n            ('cat', categorical_transformer, categorical_features)\\n        ],\\n        remainder='passthrough'  # Keep any remaining columns unchanged\\n    )\\n    \\n    print(f\\\"📊 PREPROCESSING PIPELINE CREATED\\\")\\n    print(f\\\"🔢 Numerical features ({len(numerical_features)}): {numerical_features}\\\")\\n    print(f\\\"   Transformation: StandardScaler (mean=0, std=1)\\\")\\n    print(f\\\"📝 Categorical features ({len(categorical_features)}): {categorical_features}\\\")\\n    print(f\\\"   Transformation: OneHotEncoder (binary encoding)\\\")\\n    \\n    # Estimate the output dimensionality\\n    print(f\\\"\\\\n📊 ESTIMATED OUTPUT DIMENSIONS\\\")\\n    print(\\\"-\\\" * 30)\\n    \\n    # Calculate numerical features dimension\\n    num_dims = len(numerical_features)\\n    print(f\\\"🔢 Numerical features: {num_dims} dimensions\\\")\\n    \\n    # Calculate categorical features dimension\\n    cat_dims = 0\\n    for feature in categorical_features:\\n        unique_vals = X[feature].nunique()\\n        cat_dims += unique_vals\\n        print(f\\\"📝 {feature}: {unique_vals} categories → {unique_vals} dimensions\\\")\\n    \\n    total_dims = num_dims + cat_dims\\n    print(f\\\"📊 Total estimated dimensions: {total_dims}\\\")\\n    \\n    # Create the model based on the number of targets\\n    if len(y.columns) == 1:\\n        # Single target - use regular RandomForestClassifier\\n        model = Pipeline(steps=[\\n            ('preprocessor', preprocessor),\\n            ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\\n        ])\\n        print(f\\\"\\\\n🤖 MODEL CREATED: Single-target RandomForestClassifier\\\")\\n    else:\\n        # Multiple targets - use MultiOutputClassifier\\n        model = Pipeline(steps=[\\n            ('preprocessor', preprocessor),\\n            ('classifier', MultiOutputClassifier(RandomForestClassifier(n_estimators=100, random_state=42)))\\n        ])\\n        print(f\\\"\\\\n🤖 MODEL CREATED: Multi-target RandomForestClassifier\\\")\\n    \\n    print(f\\\"📊 Target columns: {y.columns.tolist()}\\\")\\n    print(f\\\"📊 Number of targets: {len(y.columns)}\\\")\\n    \\n    # Model parameters\\n    print(f\\\"\\\\n⚙️ MODEL PARAMETERS\\\")\\n    print(\\\"-\\\" * 30)\\n    print(f\\\"Algorithm: Random Forest\\\")\\n    print(f\\\"Number of estimators: 100\\\")\\n    print(f\\\"Random state: 42\\\")\\n    print(f\\\"Multi-output: {'Yes' if len(y.columns) > 1 else 'No'}\\\")\\n    \\n    # Set flag for next steps\\n    PIPELINE_CREATED = True\\n    print(f\\\"\\\\n✅ Preprocessing pipeline created successfully!\\\")\\n    \\nelse:\\n    print(\\\"⚠️  Skipping pipeline creation - Feature types not defined\\\")\\n    PIPELINE_CREATED = False\\n    model = None\\n    preprocessor = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2d4939",
   "metadata": {},
   "source": [
    "## 14. Conclusions and Recommendations {#conclusions}\n",
    "\n",
    "Let's summarize our findings and provide recommendations for the JEE College Prediction project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1668a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📝 JEE COLLEGE PREDICTION - ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Summary of the analysis\n",
    "if DATA_AVAILABLE:\n",
    "    print(f\"✅ Data Analysis Completed Successfully!\")\n",
    "    print(f\"📊 Dataset Shape: {final_df.shape}\")\n",
    "    \n",
    "    if FEATURES_DEFINED:\n",
    "        print(f\"🎯 Features Defined: {len(selected_features)} features\")\n",
    "        print(f\"🎯 Targets Defined: {len(selected_targets)} targets\")\n",
    "    \n",
    "    if TARGETS_ENCODED:\n",
    "        print(f\"🔢 Target Encoding: Completed\")\n",
    "    \n",
    "    if FEATURE_TYPES_DEFINED:\n",
    "        print(f\"🏷️ Feature Types: {len(numerical_features)} numerical, {len(categorical_features)} categorical\")\n",
    "    \n",
    "    if PIPELINE_CREATED:\n",
    "        print(f\"🔧 ML Pipeline: Created and ready for training\")\n",
    "    \n",
    "    print(f\"\\n🎯 KEY FINDINGS:\")\n",
    "    print(f\"=\" * 40)\n",
    "    print(f\"📊 Data Quality: {'Good' if final_df.isnull().sum().sum() == 0 else 'Needs attention'}\")\n",
    "    print(f\"🔢 Numerical Features: {numerical_features if 'numerical_features' in locals() else 'Not defined'}\")\n",
    "    print(f\"📝 Categorical Features: {categorical_features if 'categorical_features' in locals() else 'Not defined'}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Data analysis incomplete - No data available\")\n",
    "\n",
    "print(f\"\\n💡 RECOMMENDATIONS:\")\n",
    "print(f\"=\" * 40)\n",
    "print(f\"1. 📊 Data Collection: Ensure comprehensive data collection for better predictions\")\n",
    "print(f\"2. 🔍 Feature Engineering: Consider creating additional features like:\")\n",
    "print(f\"   - Rank percentiles\")\n",
    "print(f\"   - Historical admission trends\")\n",
    "print(f\"   - Institute rankings\")\n",
    "print(f\"   - Branch popularity scores\")\n",
    "print(f\"3. 🤖 Model Improvement: Experiment with different algorithms:\")\n",
    "print(f\"   - XGBoost for better performance\")\n",
    "print(f\"   - Neural Networks for complex patterns\")\n",
    "print(f\"   - Ensemble methods for robust predictions\")\n",
    "print(f\"4. ✅ Model Validation: Implement cross-validation and time-series validation\")\n",
    "print(f\"5. 🚀 Deployment: Create a web application or API for real-time predictions\")\n",
    "print(f\"6. 📈 Monitoring: Set up model performance monitoring and retraining\")\n",
    "\n",
    "print(f\"\\n📋 NEXT STEPS:\")\n",
    "print(f\"=\" * 40)\n",
    "print(f\"1. 🏋️ Complete model training with train/test split\")\n",
    "print(f\"2. 📊 Evaluate model performance using appropriate metrics\")\n",
    "print(f\"3. 🔍 Analyze feature importance and model interpretability\")\n",
    "print(f\"4. 💾 Save the trained model for deployment\")\n",
    "print(f\"5. 📚 Create comprehensive documentation\")\n",
    "print(f\"6. 🧪 Set up automated testing and validation\")\n",
    "print(f\"7. 🌐 Deploy the model as a web service\")\n",
    "\n",
    "print(f\"\\n📋 DATA SCIENCE BEST PRACTICES FOLLOWED:\")\n",
    "print(f\"=\" * 40)\n",
    "print(f\"✅ Comprehensive data exploration and analysis\")\n",
    "print(f\"✅ Proper handling of missing values\")\n",
    "print(f\"✅ Robust data cleaning and preprocessing\")\n",
    "print(f\"✅ Feature engineering and selection\")\n",
    "print(f\"✅ Target encoding for categorical variables\")\n",
    "print(f\"✅ Scalable preprocessing pipeline\")\n",
    "print(f\"✅ Error handling and validation\")\n",
    "print(f\"✅ Clear documentation and visualization\")\n",
    "\n",
    "print(f\"\\n🎯 PROJECT STATUS:\")\n",
    "print(f\"=\" * 40)\n",
    "if DATA_AVAILABLE and FEATURES_DEFINED and TARGETS_ENCODED and FEATURE_TYPES_DEFINED and PIPELINE_CREATED:\n",
    "    print(f\"✅ READY FOR MODEL TRAINING AND EVALUATION\")\n",
    "    print(f\"📈 All preprocessing steps completed successfully\")\n",
    "    print(f\"🚀 Pipeline is ready for production use\")\n",
    "else:\n",
    "    print(f\"⚠️  INCOMPLETE - Some steps need attention\")\n",
    "    print(f\"📝 Please review the analysis and address any issues\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"📊 JEE COLLEGE PREDICTION - ANALYSIS COMPLETE\")\n",
    "print(f\"🎯 Thank you for using this comprehensive analysis notebook!\")\n",
    "print(f\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
